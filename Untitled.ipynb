{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fns import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_conv_net(x, keep_prob, channels, n_class, layers=3, features_root=16, filter_size=3, \n",
    "                    pool_size=2, summaries=True):\n",
    "    \"\"\"\n",
    "    Creates a new convolutional unet for the given parametrization.\n",
    "    \n",
    "    :param x: input tensor, shape [?,nx,ny,nz,channels]\n",
    "    :param keep_prob: dropout probability tensor\n",
    "    :param channels: number of channels in the input image\n",
    "    :param n_class: number of output labels\n",
    "    :param layers: number of layers in the net\n",
    "    :param features_root: number of features in the first layer\n",
    "    :param filter_size: size of the convolution filter\n",
    "    :param pool_size: size of the max pooling operation\n",
    "    :param summaries: Flag if summaries should be created\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(\"Layers: {layers}, FeaturesRoot: {features_root}, ConvolutionSize: {filter_size}*{filter_size}*{filter_size}, PoolingSize: {pool_size}*{pool_size}*{pool_size}\"\\\n",
    "                 .format(layers=layers, features_root=features_root, filter_size=filter_size, pool_size=pool_size))\n",
    "    \n",
    "    in_node = x\n",
    "    batch_size = tf.shape(x)[0] \n",
    "    weights = []\n",
    "    biases = []\n",
    "    convs = []\n",
    "    pools = OrderedDict()\n",
    "    deconv = OrderedDict()\n",
    "    dw_h_convs = OrderedDict()\n",
    "    up_h_convs = OrderedDict()\n",
    "    in_size = 144\n",
    "    size = in_size\n",
    "\n",
    "   \n",
    "    # down layers\n",
    "    with tf.name_scope('going_down'):\n",
    "        for layer in range(0, layers):\n",
    "            with tf.name_scope('layer_down_%d'%layer):\n",
    "                features = 2**layer*features_root\n",
    "                stddev = 1 / (filter_size**3 * features)\n",
    "                if layer == 0:\n",
    "                    w1 = weight_variable([filter_size, filter_size, filter_size, channels, features], stddev)\n",
    "                else:\n",
    "                    w1 = weight_variable([filter_size, filter_size, filter_size, features//2, features], stddev)\n",
    "                w2 = weight_variable([filter_size, filter_size, filter_size, features, features], stddev)\n",
    "                b1 = bias_variable([features])\n",
    "                b2 = bias_variable([features])\n",
    "                \n",
    "                conv1 = conv3d(in_node, w1, keep_prob)\n",
    "                tmp_h_conv = tf.nn.elu(conv1 + b1)\n",
    "                conv2 = conv3d(tmp_h_conv, w2, keep_prob)\n",
    "                dw_h_convs[layer] = tf.nn.elu(conv2 + b2)\n",
    "                \n",
    "                logging.info(\"Down Convoltion Layer: {layer} Size: {size}\".format(layer=layer,size=dw_h_convs[layer].get_shape()))\n",
    "                \n",
    "                weights.append((w1, w2))\n",
    "                biases.append((b1, b2))\n",
    "                convs.append((conv1, conv2))\n",
    "\n",
    "                size -= 4    \n",
    "                if layer < layers-1:\n",
    "                    pools[layer] = max_pool(dw_h_convs[layer], pool_size)\n",
    "                    in_node = pools[layer]\n",
    "                    size /= 2    \n",
    "        \n",
    "    in_node = dw_h_convs[layers-1]\n",
    "        \n",
    "    # up layers\n",
    "    with tf.name_scope('going_up'):\n",
    "        for layer in range(layers-2, -1, -1):   \n",
    "            with tf.name_scope('layer_up_%d'%layer):\n",
    "                features = 2**(layer+1)*features_root\n",
    "                stddev = 1 / (filter_size**3 * features)\n",
    "\n",
    "                wd = weight_variable_devonc([pool_size, pool_size, pool_size, features//2, features], stddev)\n",
    "                bd = bias_variable([features//2])\n",
    "                h_deconv = tf.nn.elu(deconv3d(in_node, wd, pool_size) + bd)\n",
    "                h_deconv_concat = crop_and_concat(dw_h_convs[layer], h_deconv)    \n",
    "                deconv[layer] = h_deconv_concat\n",
    "\n",
    "                w1 = weight_variable([filter_size, filter_size, filter_size, features, features//2], stddev)\n",
    "                w2 = weight_variable([filter_size, filter_size, filter_size, features//2, features//2], stddev)\n",
    "                b1 = bias_variable([features//2])\n",
    "                b2 = bias_variable([features//2])\n",
    "\n",
    "                conv1 = conv3d(h_deconv_concat, w1, keep_prob)\n",
    "                h_conv = tf.nn.elu(conv1 + b1)\n",
    "                conv2 = conv3d(h_conv, w2, keep_prob)\n",
    "                in_node = tf.nn.elu(conv2 + b2)\n",
    "                up_h_convs[layer] = in_node\n",
    "                \n",
    "                logging.info(\"Up Convoltion Layer: {layer} Size: {size}\".format(layer=layer,\n",
    "                                                                                size=tf.shape(dw_h_convs[layer])))\n",
    "                \n",
    "                weights.append((w1, w2))\n",
    "                biases.append((b1, b2))\n",
    "                convs.append((conv1, conv2))\n",
    "\n",
    "                size *= 2\n",
    "                size -= 4\n",
    "\n",
    "    # Output Map\n",
    "    with tf.name_scope('output_map'):\n",
    "        #stddev = 1 / (features_root)\n",
    "        weight = weight_variable([1, 1, 1, features_root, 1], stddev)\n",
    "        bias = bias_variable([1])\n",
    "        conv = conv3d(in_node, weight, tf.constant(1.0))\n",
    "        output_map = tf.nn.sigmoid(conv + bias)\n",
    "        up_h_convs[\"out\"] = output_map\n",
    "        logging.info(\"Output map shape {size}, offset {offset}\".format(size=output_map.get_shape(), offset=int(in_size-size)))\n",
    "\n",
    "        if summaries:\n",
    "\n",
    "            for k in dw_h_convs.keys():\n",
    "                tf.summary.histogram(\"dw_convolution_%03d\"%k + '/activations', dw_h_convs[k])\n",
    "\n",
    "            for k in up_h_convs.keys():\n",
    "                tf.summary.histogram(\"up_convolution_%s\"%k + '/activations', up_h_convs[k])\n",
    "\n",
    "        variables = []\n",
    "        for w1,w2 in weights:\n",
    "            variables.append(w1)\n",
    "            variables.append(w2)\n",
    "\n",
    "        for b1,b2 in biases:\n",
    "            variables.append(b1)\n",
    "            variables.append(b2)\n",
    "        \n",
    "        variables.append(weight)\n",
    "        variables.append(bias)\n",
    "        \n",
    "    return output_map, variables, int(in_size - size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Using split volumes\n",
      "Number of training data used: 415\n",
      "Number of validation data used: 93\n",
      "Number of testing data used: 83\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 5 but is rank 4 for 'going_down/layer_down_0/Conv3D' (op: 'Conv3D') with input shapes: [2,148,148,148], [3,3,3,1,16].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.4.0/envs/needles/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    670\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    672\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.4.0/envs/needles/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.4.0/envs/needles/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 5 but is rank 4 for 'going_down/layer_down_0/Conv3D' (op: 'Conv3D') with input shapes: [2,148,148,148], [3,3,3,1,16].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c1ce7d556fe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_conv_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Offset: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Actual Output Shape: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3dd446fcf013>\u001b[0m in \u001b[0;36mcreate_conv_net\u001b[0;34m(x, keep_prob, channels, n_class, layers, features_root, filter_size, pool_size, summaries)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mtmp_h_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_h_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Projects/pw25/ziyang/3d_unet/fns/functions.py\u001b[0m in \u001b[0;36mconv3d\u001b[0;34m(x, W, keep_prob_)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mconv_3d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'VALID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.4.0/envs/needles/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv3d\u001b[0;34m(input, filter, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m    524\u001b[0m   result = _op_def_lib.apply_op(\"Conv3D\", input=input, filter=filter,\n\u001b[1;32m    525\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    527\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.4.0/envs/needles/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.4.0/envs/needles/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2506\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.4.0/envs/needles/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1871\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1874\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.4.0/envs/needles/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.4.0/envs/needles/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.4.0/envs/needles/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 5 but is rank 4 for 'going_down/layer_down_0/Conv3D' (op: 'Conv3D') with input shapes: [2,148,148,148], [3,3,3,1,16]."
     ]
    }
   ],
   "source": [
    "# load data entirely into memory 🙁\n",
    "dataprovider = ImageDataProvider(split_vol=True, check_vol=True)\n",
    "with open('preprocessing/training_subvolumes_synth.txt', 'r') as f:\n",
    "    files_training = f.read().splitlines()\n",
    "with open('preprocessing/training_subvolumes.txt', 'r') as f:\n",
    "    files_training += f.read().splitlines()\n",
    "dataprovider.training_data_files = files_training\n",
    "\n",
    "features, labels = dataprovider._load_data_and_label(files_training, batch_size=5)\n",
    "\n",
    "batch_size = 1\n",
    "def data_iterator():\n",
    "    \"\"\" A simple data iterator \"\"\"\n",
    "    batch_idx = 0\n",
    "    while True:\n",
    "        # shuffle labels and features\n",
    "        idxs = np.arange(0, len(features))\n",
    "        np.random.shuffle(idxs)\n",
    "        shuf_features = features[idxs]\n",
    "        shuf_labels = labels[idxs]\n",
    "        for batch_idx in range(0, len(features), batch_size):\n",
    "            images_batch = shuf_features[batch_idx:batch_idx + batch_size] / 255.\n",
    "            images_batch = images_batch.astype(\"float32\")\n",
    "            labels_batch = shuf_labels[batch_idx:batch_idx + batch_size]\n",
    "            yield images_batch, crop_to_shape(labels_batch, (60,60,60))\n",
    "\n",
    "class CustomRunner(object):\n",
    "    \"\"\"\n",
    "    This class manages the the background threads needed to fill\n",
    "        a queue full of data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dataX = tf.placeholder(dtype=tf.float32, shape=[None, 148,148,148])\n",
    "        self.dataY = tf.placeholder(dtype=tf.uint8, shape=[None,60,60,60 ])\n",
    "        # The actual queue of data. The queue contains a vector for\n",
    "        # the mnist features, and a scalar label.\n",
    "        self.queue = tf.RandomShuffleQueue(shapes=[[ 148,148,148], [60,60,60]],\n",
    "                                           dtypes=[tf.float32, tf.uint8],\n",
    "                                           capacity=2000,\n",
    "                                           min_after_dequeue=1000)\n",
    "\n",
    "        # The symbolic operation to add data to the queue\n",
    "        # we could do some preprocessing here or do it in numpy. In this example\n",
    "        # we do the scaling in numpy\n",
    "        self.enqueue_op = self.queue.enqueue_many([self.dataX, self.dataY])\n",
    "\n",
    "    def get_inputs(self):\n",
    "        \"\"\"\n",
    "        Return's tensors containing a batch of images and labels\n",
    "        \"\"\"\n",
    "        images_batch, labels_batch = self.queue.dequeue_many(2)\n",
    "        return images_batch, labels_batch\n",
    "\n",
    "    def thread_main(self, sess):\n",
    "        \"\"\"\n",
    "        Function run on alternate thread. Basically, keep adding data to the queue.\n",
    "        \"\"\"\n",
    "        for dataX, dataY in data_iterator():\n",
    "            sess.run(self.enqueue_op, feed_dict={self.dataX:dataX, self.dataY:dataY})\n",
    "\n",
    "    def start_threads(self, sess, n_threads=1):\n",
    "        \"\"\" Start background threads to feed queue \"\"\"\n",
    "        threads = []\n",
    "        for n in range(n_threads):\n",
    "            t = threading.Thread(target=self.thread_main, args=(sess,))\n",
    "            t.daemon = True # thread will close when parent quits\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "        return threads\n",
    "\n",
    "# Doing anything with data on the CPU is generally a good idea.\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    custom_runner = CustomRunner()\n",
    "    images_batch, labels_batch = custom_runner.get_inputs()\n",
    "\n",
    "# simple model\n",
    "\n",
    "y = labels_batch\n",
    "logits, variables, offset = create_conv_net(images_batch, 0.75, 1, 1)\n",
    "logging.info(\"Offset: {}\".format(self.offset))\n",
    "logging.info(\"Actual Output Shape: {}\".format(logits.get_shape()))\n",
    "logging.info(\"Desired Output Shape: {}\".format(y.get_shape()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for monitoring\n",
    "loss_mean = tf.reduce_mean(loss)\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=8))\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "# start the tensorflow QueueRunner's\n",
    "tf.train.start_queue_runners(sess=sess)\n",
    "# start our custom queue runner's threads\n",
    "custom_runner.start_threads(sess)\n",
    "\n",
    "while True:\n",
    "    _, loss_val = sess.run([train_op, loss_mean])\n",
    "    print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
