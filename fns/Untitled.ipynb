{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from fns.utils import *\n",
    "from fns.functions import *\n",
    "from fns.dataprovider import *\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 10.55it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (6, 148, 148, 148, 1)\n",
      "b'\\n%/job:localhost/replica:0/task:0/cpu:0\\x12\\tlocalhost\\x1a\\x0f_38_Iterator_38 \\xa4\\xd6\\xcb\\xd1\\xa6\\xeb\\xb2\\xc1\\xc7\\x01*.N10tensorflow12_GLOBAL__N_116IteratorResourceE'\n",
      "b'\\n%/job:localhost/replica:0/task:0/cpu:0\\x12\\tlocalhost\\x1a\\x0f_38_Iterator_38 \\xa4\\xd6\\xcb\\xd1\\xa6\\xeb\\xb2\\xc1\\xc7\\x01*.N10tensorflow12_GLOBAL__N_116IteratorResourceE'\n",
      "(tf.float32, tf.float32)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-73e1b1de7555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m#store_prediction(sess, val_x, val_y, \"epoch_%s\" % epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model {}.cpkt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_path' is not defined"
     ]
    }
   ],
   "source": [
    "keep_prob = 0.75\n",
    "channels = 1\n",
    "n_class = 1\n",
    "training_iters = 6\n",
    "batch_size = 3\n",
    "epochs = 10\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # training dataset\n",
    "    features, labels = load_data(training_iters)\n",
    "    features_placeholder = tf.placeholder(features.dtype, features.shape, name='features_placeholder')\n",
    "    labels_placeholder = tf.placeholder(labels.dtype, labels.shape, name='features_placeholder')\n",
    "\n",
    "    train_dataset = tf.contrib.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "    train_dataset = train_dataset.repeat()  # Repeat the input indefinitely.\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "    #testing\n",
    "    features_test, labels_test = load_data(training_iters, mode='testing')\n",
    "    features_test_placeholder = tf.placeholder(features_test.dtype, features_test.shape, name='features_test_placeholder')\n",
    "    labels_test_placeholder = tf.placeholder(labels_test.dtype, labels_test.shape, name='labels_test_placeholder')\n",
    "\n",
    "    test_dataset = tf.contrib.data.Dataset.from_tensor_slices((features_test_placeholder, labels_test_placeholder))\n",
    "    test_dataset = test_dataset.repeat()  # Repeat the input indefinitely.\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "    # define iterators\n",
    "    train_iterator = train_dataset.make_initializable_iterator()\n",
    "    print(\"Feature shape:\", features.shape)\n",
    "    sess.run(train_iterator.initializer, feed_dict={features_placeholder: features,\n",
    "                                  labels_placeholder: labels})\n",
    "    train_iterator_handle = sess.run(train_iterator.string_handle())\n",
    "    print(train_iterator_handle)\n",
    "\n",
    "    test_iterator = test_dataset.make_initializable_iterator()\n",
    "    sess.run(test_iterator.initializer, feed_dict={features_test_placeholder: features_test,\n",
    "                                  labels_test_placeholder: labels_test})\n",
    "    test_iterator_handle = sess.run(train_iterator.string_handle())\n",
    "    print(test_iterator_handle)\n",
    "    print(train_iterator.output_types)\n",
    "\n",
    "    # define handles\n",
    "    handle = tf.placeholder(tf.string, shape=[], name='handle_placeholder')\n",
    "    iterator = tf.contrib.data.Iterator.from_string_handle(\n",
    "        handle, train_iterator.output_types)\n",
    "\n",
    "    next_example, next_label = iterator.get_next()\n",
    "\n",
    "    logging.info(\"label shape %s\"%str(next_label.get_shape()))\n",
    "\n",
    "    #---------------------------\n",
    "    # unet\n",
    "    predicter, variables, offset = create_conv_net(next_example, keep_prob, channels, n_class, \n",
    "                                                   layers=4, features_root=16, filter_size=3, \n",
    "                                                   pool_size=2, summaries=True)\n",
    "    #---------------------------\n",
    "    # prediction\n",
    "    predicter_label = tf.cast(predicter >= 0.5, tf.float32)\n",
    "    correct_pred = tf.cast(tf.equal(tf.reshape(predicter_label, [-1, n_class]), \n",
    "                                    tf.reshape(next_label, [-1, n_class])), tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct_pred)\n",
    "    cost = _get_cost(predicter, next_label)\n",
    "\n",
    "    #----------------------------\n",
    "    # optimizer op\n",
    "    training_op = tf.train.AdamOptimizer(learning_rate=tf.Variable(0.001)).minimize(cost,\n",
    "                                                                                    global_step=tf.Variable(0))\n",
    "\n",
    "    accuracy = accuracy\n",
    "    predicter_label = predicter_label\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #---------------------------\n",
    "    # summary\n",
    "    summary_writer = tf.summary.FileWriter('../test/', graph=sess.graph)\n",
    "    tf.summary.scalar('loss', cost)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "#     if restore:\n",
    "#         ckpt = tf.train.get_checkpoint_state(output_path)\n",
    "#         if ckpt and ckpt.model_checkpoint_path:\n",
    "#             restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    logging.info(\"Start optimization\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for step in range((epoch * training_iters), ((epoch + 1) * training_iters)):\n",
    "            loss_step, _ = sess.run([cost,training_op], feed_dict={handle: train_iterator_handle})\n",
    "            total_loss += loss_step\n",
    "            logging.info(\"Iter {:}, Minibatch Loss= {:.4f}\".format(step, loss_step))\n",
    "            summary_loss = tf.Summary(value=[tf.Summary.Value(tag=\"loss\", simple_value=loss_step),])\n",
    "            summary_writer.add_summary(summary_loss, step)\n",
    "            summary_writer.flush()\n",
    "            if step % 10 == 0:\n",
    "                #--------------------------\n",
    "                # validation\n",
    "                loss_val = 0\n",
    "                for step_val in range(3):\n",
    "                    loss_step_val = sess.run([cost], feed_dict={handle: test_iterator_handle})\n",
    "                    logging.info(\"Iter {:}, Testing Loss= {:.4f}\".format(step_val, np.mean(loss_step_val)))\n",
    "                    loss_step += loss_step_val\n",
    "                summary_loss_val = tf.Summary(value=[tf.Summary.Value(tag=\"loss_val\", simple_value=loss_step/3),])\n",
    "                summary_writer.add_summary(summary_loss_val, step)\n",
    "                summary_writer.flush()\n",
    "                    \n",
    "\n",
    "\n",
    "        #output_epoch_stats(epoch, total_loss, training_iters, 0.001)\n",
    "\n",
    "        #store_prediction(sess, val_x, val_y, \"epoch_%s\" % epoch)\n",
    "\n",
    "        save_path = os.path.join(output_path, \"model {}.cpkt\".format(epoch))\n",
    "        save_path = save(sess, save_path)\n",
    "\n",
    "    logging.info(\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_minibatch_stats(sess, summary_writer, step, summary_op, cost, accuracy, train_iterator_handle):\n",
    "        # Calculate batch loss and accuracy\n",
    "        summary_str = sess.run([summary_op], feed_dict={handle: train_iterator_handle})\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "        summary_writer.flush()\n",
    "\n",
    "def _process_data(data):\n",
    "        data = np.clip(np.fabs(data), -np.inf, np.inf)\n",
    "        # normalization\n",
    "        # data -= np.amin(data)\n",
    "        # data /= np.amax(data)\n",
    "        data -= np.mean(data)\n",
    "        data /= np.std(data)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "def _load_file(path, dtype=np.float32, padding=None):\n",
    "        data = nrrd.read(path)[0].astype(dtype)\n",
    "        return data\n",
    "\n",
    "def load_data(min_iter=np.inf, mode='training', synth=\"\"):\n",
    "    '''\n",
    "    mode: training, testing or validation\n",
    "    '''\n",
    "    files = []\n",
    "    if synth:\n",
    "        synth_str = '_synth'\n",
    "    else:\n",
    "        synth_str = ''\n",
    "\n",
    "    # with open('preprocessing/training_subvolumes_synth.txt', 'r') as f:\n",
    "    #     files_training = f.read().splitlines()\n",
    "    # with open('preprocessing/validation_subvolumes_synth.txt', 'r') as f:\n",
    "    #     files_validation = f.read().splitlines()\n",
    "    # with open('preprocessing/testing_subvolumes_synth.txt', 'r') as f:\n",
    "    #     files_testing = f.read().splitlines()   \n",
    "\n",
    "\n",
    "    with open('../preprocessing/%s_subvolumes%s.txt' % (mode,synth), 'r') as f:\n",
    "        files = f.read().splitlines()\n",
    "    \n",
    "\n",
    "    data, target = [], []\n",
    "    for i in trange(min(min_iter,len(files))):\n",
    "        image_path = files[i]\n",
    "\n",
    "        label_name = image_path.replace('_case', '_labelmap')\n",
    "        label_name = label_name.replace('case', 'needles')\n",
    "\n",
    "        img = _load_file(image_path, np.float32, padding=\"noise\")\n",
    "        img = _process_data(img)[...,np.newaxis]\n",
    "\n",
    "        annotation = _load_file(label_name, np.float32, padding=\"zero\")\n",
    "        annotation = crop_to_shape2(annotation, (60,60,60))[...,np.newaxis]\n",
    "\n",
    "        data.append(img)\n",
    "        target.append(annotation)\n",
    "    return np.array(data), np.array(target)\n",
    "\n",
    "def _get_cost(predicter, labels):\n",
    "    \"\"\"\n",
    "    Constructs the cost function dice_coefficient.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('cost_function'):\n",
    "        logging.info('*' * 50)\n",
    "        logging.info('getting cost')\n",
    "        logging.info(\"Logits: {}\".format(predicter.get_shape()))\n",
    "        logging.info(\"Y: {}\".format(labels.get_shape()))\n",
    "        flat_predicter = tf.reshape(predicter, [-1, 1], name='flat_predicter_reshape')\n",
    "        flat_labels = tf.reshape(labels, [-1, 1], name='flat_labels_reshape')\n",
    "\n",
    "        intersection = tf.reduce_sum(flat_predicter * flat_labels)\n",
    "        union = tf.reduce_sum(flat_predicter) + tf.reduce_sum(flat_labels)\n",
    "        loss = 1 - 2 * intersection / union\n",
    "    return loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_net(x, keep_prob, channels, n_class, layers=3, features_root=16, filter_size=3, \n",
    "                    pool_size=2, summaries=True):\n",
    "    \"\"\"\n",
    "    Creates a new convolutional unet for the given parametrization.\n",
    "    \n",
    "    :param x: input tensor, shape [?,nx,ny,nz,channels]\n",
    "    :param keep_prob: dropout probability tensor\n",
    "    :param channels: number of channels in the input image\n",
    "    :param n_class: number of output labels\n",
    "    :param layers: number of layers in the net\n",
    "    :param features_root: number of features in the first layer\n",
    "    :param filter_size: size of the convolution filter\n",
    "    :param pool_size: size of the max pooling operation\n",
    "    :param summaries: Flag if summaries should be created\n",
    "    \"\"\"\n",
    "    \n",
    "    logging.info(\"Layers: {layers}, FeaturesRoot: {features_root}, ConvolutionSize: {filter_size}*{filter_size}*{filter_size}, PoolingSize: {pool_size}*{pool_size}*{pool_size}\"\\\n",
    "                 .format(layers=layers, features_root=features_root, filter_size=filter_size, pool_size=pool_size))\n",
    "    \n",
    "    in_node = x\n",
    "    batch_size = tf.shape(x)[0] \n",
    "    weights = []\n",
    "    biases = []\n",
    "    convs = []\n",
    "    pools = OrderedDict()\n",
    "    deconv = OrderedDict()\n",
    "    dw_h_convs = OrderedDict()\n",
    "    up_h_convs = OrderedDict()\n",
    "    in_size = 144\n",
    "    size = in_size\n",
    "\n",
    "   \n",
    "    # down layers\n",
    "    with tf.name_scope('going_down'):\n",
    "        for layer in range(0, layers):\n",
    "            with tf.name_scope('layer_down_%d'%layer):\n",
    "                features = 2**layer*features_root\n",
    "                stddev = 1 / (filter_size**3 * features)\n",
    "                if layer == 0:\n",
    "                    w1 = weight_variable([filter_size, filter_size, filter_size, channels, features], stddev)\n",
    "                else:\n",
    "                    w1 = weight_variable([filter_size, filter_size, filter_size, features//2, features], stddev)\n",
    "                w2 = weight_variable([filter_size, filter_size, filter_size, features, features], stddev)\n",
    "                b1 = bias_variable([features])\n",
    "                b2 = bias_variable([features])\n",
    "                \n",
    "                conv1 = conv3d(in_node, w1, keep_prob)\n",
    "                tmp_h_conv = tf.nn.elu(conv1 + b1)\n",
    "                conv2 = conv3d(tmp_h_conv, w2, keep_prob)\n",
    "                dw_h_convs[layer] = tf.nn.elu(conv2 + b2)\n",
    "                \n",
    "                logging.info(\"Down Convoltion Layer: {layer} Size: {size}\".format(layer=layer,size=dw_h_convs[layer].get_shape()))\n",
    "                \n",
    "                weights.append((w1, w2))\n",
    "                biases.append((b1, b2))\n",
    "                convs.append((conv1, conv2))\n",
    "\n",
    "                size -= 4    \n",
    "                if layer < layers-1:\n",
    "                    pools[layer] = max_pool(dw_h_convs[layer], pool_size)\n",
    "                    in_node = pools[layer]\n",
    "                    size /= 2    \n",
    "        \n",
    "    in_node = dw_h_convs[layers-1]\n",
    "        \n",
    "    # up layers\n",
    "    with tf.name_scope('going_up'):\n",
    "        for layer in range(layers-2, -1, -1):   \n",
    "            with tf.name_scope('layer_up_%d'%layer):\n",
    "                features = 2**(layer+1)*features_root\n",
    "                stddev = 1 / (filter_size**3 * features)\n",
    "\n",
    "                wd = weight_variable_devonc([pool_size, pool_size, pool_size, features//2, features], stddev)\n",
    "                bd = bias_variable([features//2])\n",
    "                h_deconv = tf.nn.elu(deconv3d(in_node, wd, pool_size) + bd)\n",
    "                h_deconv_concat = crop_and_concat(dw_h_convs[layer], h_deconv)    \n",
    "                deconv[layer] = h_deconv_concat\n",
    "\n",
    "                w1 = weight_variable([filter_size, filter_size, filter_size, features, features//2], stddev)\n",
    "                w2 = weight_variable([filter_size, filter_size, filter_size, features//2, features//2], stddev)\n",
    "                b1 = bias_variable([features//2])\n",
    "                b2 = bias_variable([features//2])\n",
    "\n",
    "                conv1 = conv3d(h_deconv_concat, w1, keep_prob)\n",
    "                h_conv = tf.nn.elu(conv1 + b1)\n",
    "                conv2 = conv3d(h_conv, w2, keep_prob)\n",
    "                in_node = tf.nn.elu(conv2 + b2)\n",
    "                up_h_convs[layer] = in_node\n",
    "                \n",
    "                logging.info(\"Up Convoltion Layer: {layer} Size: {size}\".format(layer=layer,\n",
    "                                                                                size=tf.shape(dw_h_convs[layer])))\n",
    "                \n",
    "                weights.append((w1, w2))\n",
    "                biases.append((b1, b2))\n",
    "                convs.append((conv1, conv2))\n",
    "\n",
    "                size *= 2\n",
    "                size -= 4\n",
    "\n",
    "    # Output Map\n",
    "    with tf.name_scope('output_map'):\n",
    "        #stddev = 1 / (features_root)\n",
    "        weight = weight_variable([1, 1, 1, features_root, 1], stddev)\n",
    "        bias = bias_variable([1])\n",
    "        conv = conv3d(in_node, weight, tf.constant(1.0))\n",
    "        output_map = tf.nn.sigmoid(conv + bias)\n",
    "        up_h_convs[\"out\"] = output_map\n",
    "        logging.info(\"Output map shape {size}, offset {offset}\".format(size=output_map.get_shape(), offset=int(in_size-size)))\n",
    "\n",
    "        if summaries:\n",
    "            for k in dw_h_convs.keys():\n",
    "                tf.summary.histogram(\"dw_convolution_%03d\"%k + '/activations', dw_h_convs[k])\n",
    "\n",
    "            for k in up_h_convs.keys():\n",
    "                tf.summary.histogram(\"up_convolution_%s\"%k + '/activations', up_h_convs[k])\n",
    "\n",
    "        variables = []\n",
    "        for w1,w2 in weights:\n",
    "            variables.append(w1)\n",
    "            variables.append(w2)\n",
    "\n",
    "        for b1,b2 in biases:\n",
    "            variables.append(b1)\n",
    "            variables.append(b2)\n",
    "        \n",
    "        variables.append(weight)\n",
    "        variables.append(bias)\n",
    "        \n",
    "    return output_map, variables, int(in_size - size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
